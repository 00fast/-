{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_table('/path/train.txt')\n",
    "test_data = pd.read_table('/path/test.txt')\n",
    "\n",
    "train_data.head() # 상위 5개 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any())\n",
    "test_data = test_data.dropna(how = 'any')\n",
    "print(test_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거 \n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections.abc import Iterable\n",
    "\n",
    "# 동의어 처리\n",
    "with open('/content/thesaurus_dic.json', 'r', encoding='utf-8') as f:\n",
    "    thesaurus = json.load(f)\n",
    "\n",
    "# 학습 및 테스트 데이터 내용 및 라벨 준비\n",
    "train_texts = train_data[\"document\"]\n",
    "test_texts = test_data[\"document\"]\n",
    "train_labels = train_data[\"label\"]\n",
    "test_labels = test_data[\"label\"]\n",
    "\n",
    "# 바뀐 단어들이 [] 형태로 들어가지 않기 위해 사용\n",
    "def flatten(lis):\n",
    "  for item in lis:\n",
    "    if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "      for x in flatten(item):\n",
    "        yield x\n",
    "    else:\n",
    "      yield item\n",
    "\n",
    "# 토크나이저 및 불용어 제거\n",
    "def preprocess_stop_words(text): \n",
    "  okt = Okt()\n",
    "  text = text.strip()\n",
    "  text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "  # 불용어 사전 txt 파일 이용해 삭제\n",
    "  with open('/content/stop_words.txt', 'r', encoding='utf-8') as f:\n",
    "    custom_stopwords = [word.strip() for word in f]\n",
    "    text = re.sub('[^A-Za-z0-9가-힣]', ' ', text)  # 특문 제거\n",
    "\n",
    "    # 토큰화\n",
    "    tokens = okt.morphs(text)\n",
    "\n",
    "    # Thesaurus 적용\n",
    "    replaced_words = [thesaurus.get(word, word) for word in tokens]\n",
    "    replaced_words = [' '.join(words) if isinstance(words, list) else words for words in replaced_words]\n",
    "\n",
    "    # 불용어 제거\n",
    "    mecab_word = [word for word in replaced_words if word not in custom_stopwords]\n",
    "\n",
    "    text = ' '.join(mecab_word)\n",
    "  return text\n",
    "\n",
    "X_train = [preprocess_stop_words(text) for text in train_texts]\n",
    "X_test = [preprocess_stop_words(text) for text in test_texts]\n",
    "Y_train = train_labels\n",
    "Y_test = test_labels"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
